{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QL07Mt8_he2O",
        "TIS7nyvwgy8p",
        "AgrQxISAgzP5",
        "b67Qs9XCgzhW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZpIAXTLMYD6"
      },
      "source": [
        "# Language models\n",
        "\n",
        "Which word in the sequence is more likely:\n",
        "\n",
        "The train arrived at the\n",
        "* north \n",
        "* railway station\n",
        "\n",
        "Which sequence is more likely:\n",
        "* The train arrived at the station\n",
        "* The station arrived at the train\n",
        "\n",
        "The language model [language model, LM] allows you to estimate the probability of the next word in the sequence $P(w_n | w_1, \\ldots, w_{n-1})$ and estimate the probability of the entire sequence of words $P(w_1, \\ldots, w_n)$.\n",
        "\n",
        "### Applications:\n",
        "\n",
        "#### Tasks where complex and noisy input needs to be processed: \n",
        "* Speech recognition, \n",
        "* Recognition of scanned and handwritten texts;\n",
        "* Correction of typos\n",
        "* Machine translation\n",
        "* Tip when typing\n",
        "\n",
        "#### Types of models:\n",
        "* Countable models\n",
        "    - Markov chains\n",
        "* Neural network models, usually recurrent neural networks with LSTM/GRU \n",
        "* Seq2Seq architectures\n",
        "\n",
        "\n",
        "## The $n$-gram model\n",
        "Let $w_{1:n}=w_1,\\ldots,w_m$ be a sequence of words.\n",
        "\n",
        "Chain rule: \n",
        "\n",
        "$$P(w_{1:m}) = P(w_1) P(w_2 | w_1) P(w_3 | w_{1:2}) \\ldots P(w_m | w_{1:m-1}) = \\prod_{k=1}^{m} P(w_k | w_{1:k-1}) $$\n",
        "\n",
        "But evaluating $P(w_k | w_{1:k-1})$ is not easier!\n",
        "\n",
        "We move on to $n$-grams: $P(w_{i+1} | w_{1:i}) \\approx P(w_{i+1} | w_{i-n:i}) $ , which means that we take into account $n-1$ the previous word.\n",
        "\n",
        "### Model\n",
        "\n",
        "* _unigram:_ $P(w_k)$\n",
        "\n",
        "* _bigram:_ $P(w_k | w_{k-1})$\n",
        "\n",
        "* _trigram:_ $P(w_k |w_{k-1} w_{k-2})$\n",
        "\n",
        "I.e. we use Markov assumptions about the length of the stored chain.\n",
        "\n",
        "* The probability of the next word in the sequence: $ P(w_{i+1} | w_{1:i}) \\approx P(w_{in:i}) $\n",
        "* The probability of the whole sequence of words, $P(w_{1:n}) = \\prod_{k=1}^l P(w_k | w_{k-n+1: k-1}) $\n",
        "\n",
        "\n",
        "## The $n$-gram model quality estimation\n",
        "\n",
        "__Perplexity:__ How good is the model at predicting the sample. The lower the perplexy value, the better.\n",
        "\n",
        "$PP(\\texttt{LM}) = 2 ^ {-\\frac{1}{m} \\log_2 \\texttt{LM} (w_i | w_{1:i-1})}$\n",
        "\n",
        "![](https://miro.medium.com/max/1050/1*J5kBR7XsQqRiu0p_CZEk1w.png)\n",
        "\n",
        "We want our model to assign high probabilities to sentences that are real and syntactically correct, and low probabilities to fake, incorrect, or highly infrequent sentences. Assuming our dataset is made of sentences that are in fact real and correct, this means that the best model will be the one that assigns the highest probability to the test set. Intuitively, if a model assigns a high probability to the test set, it means that it is not surprised to see it (it’s not perplexed by it), which means that it has a good understanding of how the language works.\n",
        "\n",
        "MLE probability estimation:\n",
        "\n",
        "$ P_{MLE}(w_k | w_{k-n+1:k-1}) = \\frac{\\texttt{count}(w_{k-n+1:k-1} w_k )}{\\texttt{count}(w_{k-n+1:k-1} )} $\n",
        "\n",
        "In the bigram model:\n",
        "\n",
        "$ P_{MLE}(w_k | w_{k-1}) = \\frac{\\texttt{count}(w_{k-1} w_k )}{\\texttt{count}(w_{k-1} )} $\n",
        "\n",
        "The problem of zero probabilities arises!\n",
        "\n",
        "Additive Laplace smoothing\n",
        "$ P(w_k | w_{k-1}) = \\frac{\\texttt{count}(w_{k-1} w_k ) + \\alpha}{\\texttt{count}(w_{k-1} ) + \\alpha |V|} $\n",
        "\n",
        "\n",
        "## Example\n",
        "\n",
        "![](https://github.com/artemovae/ML-for-compling/raw/668293ddcf40ef30461c45676ec1931c69551553/2018/img/aib.png)\n",
        "\n",
        "BOS А и Б сидели на трубе EOS\n",
        "\n",
        "BOS А упало Б пропало EOS\n",
        "\n",
        "BOS что осталось на трубе EOS\n",
        "\n",
        "$P($ и $| $ A $) = \\frac{1}{2}$\n",
        "\n",
        "$P($ Б $| $ и $) = \\frac{1}{1}$\n",
        "\n",
        "$P($ трубе $| $ на $) = \\frac{2}{2}$\n",
        "\n",
        "$P($ сидели $| $ Б $) = \\frac{1}{2}$\n",
        "\n",
        "$P($ на $| $ сидели $) = \\frac{1}{2}$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpZl8IrHM5tI",
        "outputId": "3f742808-7500-4a2e-bcb4-e0a30bfda7de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM_Vhc7mrE6S",
        "outputId": "1e163831-824a-4968-ff6a-d90993306ddc"
      },
      "source": [
        "import nltk\n",
        "cfreq = nltk.ConditionalFreqDist(nltk.bigrams('''BOS А и Б сидели на трубе EOS\n",
        "\n",
        "BOS А упало Б пропало EOS\n",
        "\n",
        "BOS что осталось на трубе EOS'''.split()))\n",
        "\n",
        "cprob = nltk.ConditionalProbDist(cfreq, nltk.MLEProbDist)\n",
        "print('p(А и) = %1.4f' %cprob['А'].prob('и'))\n",
        "print('p(и Б) = %1.4f' %cprob['и'].prob('Б'))\n",
        "print('p(на трубе) = %1.4f' %cprob['на'].prob('трубе'))\n",
        "print('p(Б сидели) = %1.4f' %cprob['Б'].prob('сидели'))\n",
        "print('p(сидели на) = %1.4f' %cprob['сидели'].prob('на'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p(А и) = 0.5000\n",
            "p(и Б) = 1.0000\n",
            "p(на трубе) = 1.0000\n",
            "p(Б сидели) = 0.5000\n",
            "p(сидели на) = 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rwiBCTAr4B4"
      },
      "source": [
        "**Now let us make dinosaurs!!!!!!!!!!!1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avMJquAVVkqQ",
        "outputId": "968304eb-b7d0-4515-c825-304b1fb02924"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9674 sha256=cbddd5e34369fdd6ddeec4572622ff76b096c5b333bcd355cab13fe7f637963c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "IFvhHIXaVepl",
        "outputId": "a9a91bd8-125d-46e2-8a43-ac029c4c9da8"
      },
      "source": [
        "import wget\n",
        "\n",
        "wget.download(\"https://raw.githubusercontent.com/artemovae/ML-for-compling/master/2018/dinos.txt\", \"dinos.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dinos.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuFxsiYVKSFd"
      },
      "source": [
        "import nltk\n",
        "\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBDzhvfeVW_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b83a45b-8da9-4dcd-df64-015550d5ebf3"
      },
      "source": [
        "names = ['<' + name.strip().lower() + '>' for name in open('dinos.txt').readlines()]\n",
        "print(names[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<aachenosaurus>', '<aardonyx>', '<abdallahsaurus>', '<abelisaurus>', '<abrictosaurus>', '<abrosaurus>', '<abydosaurus>', '<acanthopholis>', '<achelousaurus>', '<acheroraptor>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7kyQ4YWWh7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5346956d-c8ef-4bf9-aabd-d08952a3f628"
      },
      "source": [
        "chars = [char  for name in names for char in name]\n",
        "freq = nltk.FreqDist(chars)\n",
        "\n",
        "print(sorted(list(freq.keys())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<', '>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ud4mHY3Wt6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae1c04d2-4130-4d6c-9294-e4c0d5435db7"
      },
      "source": [
        "cfreq = nltk.ConditionalFreqDist(nltk.bigrams(chars))\n",
        "cfreq['a']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'u': 791, 'n': 347, 't': 204, 's': 171, 'l': 138, '>': 138, 'r': 124, 'c': 100, 'p': 89, 'm': 68, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPI3zlxTWx_s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56fdcd74-78a9-43dc-b68d-e2ea4c1466be"
      },
      "source": [
        "cprob = nltk.ConditionalProbDist(cfreq, nltk.MLEProbDist)\n",
        "print('p(a a) = %1.4f' %cprob['a'].prob('a'))\n",
        "print('p(a b) = %1.4f' %cprob['a'].prob('b'))\n",
        "print('p(a u) = %1.4f' %cprob['a'].prob('u'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p(a a) = 0.0044\n",
            "p(a b) = 0.0097\n",
            "p(a u) = 0.3181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0cSPJPVW3Dl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0241859-b94c-4fda-8803-1287a0f5763c"
      },
      "source": [
        "from math import log\n",
        "log(cprob['a'].prob('a')) + log(cprob['a'].prob('b')) + log(cprob['a'].prob('c'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-13.275378042275806"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHJ0tFiWX9Di",
        "outputId": "881a6265-eac2-4a7a-dcad-a008f24e5da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'a': 2487, 's': 2285, 'u': 2123, 'o': 1710, 'r': 1704, '<': 1536, '>': 1536, 'n': 1081, 'i': 944, 'e': 913, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq['a']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADyZisM5Qddf",
        "outputId": "3347ba05-cbca-40af-bcab-c80310de606f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2487"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nWZH8eyW7P5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9f221e0-a690-4724-db1b-60d65f78b8a0"
      },
      "source": [
        "l = sum([freq[char] for char in freq])\n",
        "\n",
        "def unigram_prob(char):\n",
        "    return freq[char] / l\n",
        "\n",
        "print('p(a) = %1.4f' %unigram_prob('a'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p(a) = 0.1160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_KlFlLrW9uT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dc927f1-c636-4e02-8a73-757c9f9132fe"
      },
      "source": [
        "[bi for bi in nltk.bigrams('<aachenosaurus>')]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<', 'a'),\n",
              " ('a', 'a'),\n",
              " ('a', 'c'),\n",
              " ('c', 'h'),\n",
              " ('h', 'e'),\n",
              " ('e', 'n'),\n",
              " ('n', 'o'),\n",
              " ('o', 's'),\n",
              " ('s', 'a'),\n",
              " ('a', 'u'),\n",
              " ('u', 'r'),\n",
              " ('r', 'u'),\n",
              " ('u', 's'),\n",
              " ('s', '>')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "K6VTbj6LJep-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams = [bi for bi in nltk.bigrams('aba caba baca bac')]"
      ],
      "metadata": {
        "id": "IhzfjqQQJhW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(bigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYXh61WcJuTU",
        "outputId": "397c9555-91ab-4b3b-d312-0ae975445923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ab = [bi for bi in bigrams if bi == ('a', 'b')]"
      ],
      "metadata": {
        "id": "fgiR6z6FKZ06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYdqNGpQKfVJ",
        "outputId": "69576cd0-b0d3-4607-abbb-4cce96f21865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = list('aba caba baca bac')"
      ],
      "metadata": {
        "id": "PkljrsbFLIG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfreq = nltk.ConditionalFreqDist(nltk.bigrams(chars))\n",
        "cprob = nltk.ConditionalProbDist(cfreq, nltk.MLEProbDist)\n",
        "print(f\"p(a b) = {cprob['a'].prob('b')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3Urgg6QJ4Td",
        "outputId": "b9bc1110-f7a0-4e75-ac59-6d0fd89b52a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p(a b) = 0.2857142857142857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2/7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a__2giahLynn",
        "outputId": "0ffbed20-15bf-4dd5-e5fc-085dc47677d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2857142857142857"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cvPDuXhXaxO"
      },
      "source": [
        "### Task 1. \n",
        "\n",
        "1.1 Write a function to estimate the probability of a dinosaur name.\n",
        "\n",
        "1.2 Find the most likely dinosaur name from this list."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_dino_prob(dinosaur_name):\n",
        "    prob = 1.0\n",
        "    for left, right in nltk.bigrams(dinosaur_name):\n",
        "        prob *= cprob[left].prob(right)\n",
        "    return prob"
      ],
      "metadata": {
        "id": "7bBrGQBSYUqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert estimate_dino_prob('<aachenosaurus>') > estimate_dino_prob('<aachenosauril>')"
      ],
      "metadata": {
        "id": "pBbBAtsMYUtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(reversed(sorted([(dinosaur_name, estimate_dino_prob(dinosaur_name)) for dinosaur_name in names], key=lambda x: x[1])))[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRQhkt85YVOX",
        "outputId": "06862fda-02e0-4f9d-d01a-4d3b917b6ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<talos>', 2.639985626826119e-05)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DiQ7v6zOYVbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LM43VZ9XM2a"
      },
      "source": [
        "# your code here\n",
        "def compute_probability(word):\n",
        "    whole_probability = 1.0\n",
        "    for i, j in [bi for bi in nltk.bigrams(word)]:\n",
        "        #print('p(%s %s) = %1.4f' %(i, j, cprob[i].prob(j)))\n",
        "        whole_probability *= cprob[i].prob(j)\n",
        "    return whole_probability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2Q2NnA0H-yE",
        "outputId": "abdc9e9c-b6b8-46c3-b0f0-5f524ad41052"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "probs = [(word, compute_probability(word)) for word in names]\n",
        "sorted(probs, key=lambda x: x[1], reverse=True)[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<talos>', 2.639985626826119e-05),\n",
              " ('<mei>', 2.112487234254014e-06),\n",
              " ('<elosaurus>', 1.8327456825026856e-06),\n",
              " ('<almas>', 7.361477189901781e-07),\n",
              " ('<balaur>', 6.732825877952762e-07)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oImb2MTBJ59S"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xhJUWZnXm18"
      },
      "source": [
        "### Task 2.\n",
        "\n",
        "Write a function that generates a new dinosaur given the length of the expected name."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h1u77r_kcVBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dRb615U9cVNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp2DYp0GXZn0"
      },
      "source": [
        "# your code here\n",
        "def generate_n_word(n=10):\n",
        "    new_name = \"<\"\n",
        "    for i in range(n):\n",
        "        new_name += cprob[new_name[-1]].generate()\n",
        "        print(new_name)\n",
        "    return new_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "eenU6olAZuLa",
        "outputId": "dc6bd905-03f7-49f7-f962-d34314f06fd1"
      },
      "source": [
        "generate_n_word(11)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<m\n",
            "<ma\n",
            "<man\n",
            "<mani\n",
            "<mania\n",
            "<maniah\n",
            "<maniahu\n",
            "<maniahus\n",
            "<maniahus>\n",
            "<maniahus><\n",
            "<maniahus><a\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<maniahus><a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Jq4QLeKYMBr"
      },
      "source": [
        "## Recurrent Neural Networks (RNN)\n",
        "\n",
        "The original sequence:\n",
        "\n",
        "$x_{1:n} = x_1, x_2, \\ldots, x_n$, $x_i \\in \\mathbb{R}^{d_{in}}$\n",
        "\n",
        "-----------------------------------------\n",
        "\n",
        "For each input value $x_{1:i}$, we get $y_i$ at the output:\n",
        "\n",
        "$y_i = RUN(x_{1:i})$, $y_i \\in \\mathbb{R}^{d_{out}}$\n",
        "\n",
        "-----------------------------------------\n",
        "\n",
        "For the entire sequence $x_{1:n}$:\n",
        "\n",
        "$y_{1:n} = RN^{*}(x_{1:n})$, $y_i \\in \\mathbb{R}^{d_{out}}$\n",
        "\n",
        "$R$ is a recursive activation function depending on two parameters: $x_i$ and $s_{i-1}$ (the vector of the previous state)\n",
        "\n",
        "-----------------------------------------\n",
        "\n",
        "$RNN^{*}(x_{1:n}, s_0) = y_{1:n}$\n",
        "\n",
        "$y_i = O(s_i) = g(W^{out}[s_{i} ,x_i] +b)$\n",
        "\n",
        "$s_i = R(s_{i-1}, x_i)$\n",
        "\n",
        "$s_i = R(s_{i-1}, x_i) = g(W^{hid}[s_{i-1} ,x_i] +b)$ -- concatenation $[s_{i-1}, x]$\n",
        "\n",
        "$x_i \\in \\mathbb{R}^{d_{in}}$, $y_i \\in \\mathbb{R}^{ d_{out}}$, $s_i \\in \\mathbb{R}^{d_{hid}}$\n",
        "\n",
        "$W^{head} \\in \\mathbb{R}^{(d_{in}+d_{out}) \\times d_{hid}}$, $W^{out} \\in \\mathbb{R}^{d_{hid} \\times d_{out}}$\n",
        "\n",
        "Let's build a language model based on RNN using pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyTYPnwWagrF"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pdb\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "torch.set_printoptions(linewidth=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 381 \n",
        "torch.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)"
      ],
      "metadata": {
        "id": "KgmVlbE8SSDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-_uDNimal0u"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "hidden_size = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb4JBPM4fUbE"
      },
      "source": [
        "Let us prepare a dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn_k8XFvfPVx"
      },
      "source": [
        "class DinosDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        with open('dinos.txt') as f:\n",
        "            content = f.read().lower()\n",
        "            self.vocab = sorted(set(content)) + ['<', '>']\n",
        "            self.vocab_size = len(self.vocab)\n",
        "            self.lines = content.splitlines()\n",
        "        self.ch_to_idx = {c:i for i, c in enumerate(self.vocab)}\n",
        "        self.idx_to_ch = {i:c for i, c in enumerate(self.vocab)}\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        line = self.lines[index]\n",
        "        #teacher forcing\n",
        "        x_str = '<' + line \n",
        "        y_str = line + '>' \n",
        "        x = torch.zeros([len(x_str), self.vocab_size], dtype=torch.float)\n",
        "        y = torch.empty(len(x_str), dtype=torch.long)\n",
        "        for i, (x_ch, y_ch) in enumerate(zip(x_str, y_str)):\n",
        "            x[i][self.ch_to_idx[x_ch]] = 1\n",
        "            y[i] = self.ch_to_idx[y_ch]\n",
        "        \n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJbMiCbtfSKs"
      },
      "source": [
        "trn_ds = DinosDataset()\n",
        "trn_dl = DataLoader(trn_ds, shuffle=True, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUKenXbIfbOv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c892313b-9c85-47a4-edb4-084be7e30902"
      },
      "source": [
        "trn_ds.lines[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'aardonyx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trn_ds.vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayU77VTIUBSQ",
        "outputId": "cb1b2bcc-e1cd-45ce-e72a-49b279bb16e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '<',\n",
              " '>']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trn_ds.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8N9FuUOthFf3",
        "outputId": "dd49f8e0-a8d3-4e39-d99a-080dc965a7ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKFd9PLTfc5i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a409ae4-c4c4-479a-d582-b3b5098c0d13"
      },
      "source": [
        "print(trn_ds.idx_to_ch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 27: '<', 28: '>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD_xwzA2ffX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba9b65ad-cbf2-4f7a-ffc8-2a7cb511c37e"
      },
      "source": [
        "trn_ds.vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev_MZNgufio0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6e82ef7-1581-4470-81bf-668d91b8b9ee"
      },
      "source": [
        "x, y = trn_ds[1]\n",
        "x.shape, y.shape, x, y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([9, 29]),\n",
              " torch.Size([9]),\n",
              " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]),\n",
              " tensor([ 1,  1, 18,  4, 15, 14, 25, 24, 28]))"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9rHsIVFftbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07fd5495-b054-4f08-ffec-55525dd2c1bd"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfcuCrx4fqkb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1f6b08-0e0b-4366-f243-9cdcba54c497"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1,  1, 18,  4, 15, 14, 25, 24, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-7eNpLDMeGh",
        "outputId": "eb510d10-6189-4de5-c963-6556e3ee057a"
      },
      "source": [
        "[\"<\"] +[trn_ds.idx_to_ch[i] for i in [ 1,  1, 18,  4, 15, 14, 25, 24, 28]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<', 'a', 'a', 'r', 'd', 'o', 'n', 'y', 'x', '>']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST5WS0b8f3BC"
      },
      "source": [
        "Let us create the RRN model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T_QDHgEfrw7"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        # test\n",
        "        self.i2o = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, h_prev, x):\n",
        "        h, c = self.lstm(x, h_prev)\n",
        "        h = torch.tanh(h)\n",
        "        y = self.i2o(h)\n",
        "        return (h, c), y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUYimQZUf8nj"
      },
      "source": [
        "model = RNN(trn_ds.vocab_size, hidden_size, trn_ds.vocab_size).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8bc2noGgJIf"
      },
      "source": [
        "![](https://github.com/PragmaticsLab/NLP-course-AMI/raw/0cb50728ceaa825f97d88f4e72efc954b817badc/seminars/sem4_language_models/images/dinos3.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_rCGfP7f-ci"
      },
      "source": [
        "def sample(model):\n",
        "    model.eval()\n",
        "    word_size=0\n",
        "    newline_idx = trn_ds.ch_to_idx['>']\n",
        "    with torch.no_grad():\n",
        "        h_prev = (torch.zeros([1, hidden_size], dtype=torch.float, device=device),\n",
        "                  torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
        "                  )\n",
        "        x = h_prev[0].new_zeros([1, trn_ds.vocab_size])\n",
        "        start_char_idx = trn_ds.ch_to_idx['<']\n",
        "        indices = [start_char_idx]\n",
        "        x[0, start_char_idx] = 1\n",
        "        predicted_char_idx = start_char_idx\n",
        "        \n",
        "        while predicted_char_idx != newline_idx and word_size != 50:\n",
        "            h_prev, y_pred = model(h_prev, x)\n",
        "            y_softmax_scores = torch.softmax(y_pred, dim=1)\n",
        "            \n",
        "            np.random.seed(np.random.randint(1, 5000))\n",
        "            idx = np.random.choice(np.arange(trn_ds.vocab_size), p=y_softmax_scores.cpu().numpy().ravel())\n",
        "            indices.append(idx)\n",
        "            \n",
        "            x = (y_pred == y_pred.max(1)[0]).float()\n",
        "            \n",
        "            predicted_char_idx = idx\n",
        "            \n",
        "            word_size += 1\n",
        "        \n",
        "        if word_size == 50:\n",
        "            indices.append(newline_idx)\n",
        "    return indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hlddHN6gN-M"
      },
      "source": [
        "def print_sample(sample_idxs):\n",
        "    [print(trn_ds.idx_to_ch[x], end ='') for x in sample_idxs]\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oga_CEQGgQWH"
      },
      "source": [
        "Let us train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITyuIlNhgOW1"
      },
      "source": [
        "def train_one_epoch(model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    for line_num, (x, y) in enumerate(trn_dl):\n",
        "        loss = 0\n",
        "        optimizer.zero_grad()\n",
        "        h_prev = (torch.zeros([1, hidden_size], dtype=torch.float, device=device),\n",
        "                  torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
        "                  )\n",
        "\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        for i in range(x.shape[1]):\n",
        "            h_prev, y_pred = model(h_prev, x[:, i])\n",
        "            loss += loss_fn(y_pred, y[:, i])\n",
        "            \n",
        "        if (line_num+1) % 100 == 0: \n",
        "            print_sample(sample(model))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    perplexity = torch.exp(loss)\n",
        "    print(f'Perplexity:{perplexity}') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrP-JhJtgW_6"
      },
      "source": [
        "def train(model, loss_fn, optimizer, dataset='dinos', epochs=1):\n",
        "    for e in range(1, epochs+1):\n",
        "        print('Epoch:{}'.format(e))\n",
        "        train_one_epoch(model, loss_fn, optimizer)\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fb_0WWlgZjT",
        "outputId": "7fa8c90e-90f9-4556-f040-ab1e12cd5a43"
      },
      "source": [
        "train(model, loss_fn, optimizer, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1\n",
            "<i\n",
            "erb\n",
            "pcseancuvnnyaouko<nopsapsfh>\n",
            "<>\n",
            "<dlsuaupusanoo>\n",
            "<uessrauap>\n",
            "<rxcuuurmuus>\n",
            "<tnrsuaosar>\n",
            "<ashuabsoasn>\n",
            "<hiatanusuc>\n",
            "<tsnerrn<uru>\n",
            "<ualnaiuaxr>\n",
            "<acrnaneslra>\n",
            "<alvruauu>\n",
            "<lerrn>\n",
            "<smrstaoearur>\n",
            "<guaeooaossssa>\n",
            "Perplexity:3719522156544.0\n",
            "\n",
            "Epoch:2\n",
            "<alvpteousus>\n",
            "<rmysosrudusarsaur>\n",
            "<adrnaaususo>\n",
            "<amurugourus>\n",
            "<rmysisruarros>\n",
            "<asgsagiugus>\n",
            "<hkcucaurus>\n",
            "<ttkiosaurus>\n",
            "<tainanhirusaus>\n",
            "<anashoaurus>\n",
            "<ucusopaurut>\n",
            "<krttanarrus>\n",
            "<guagooaurus>\n",
            "<aucitcurus>\n",
            "<lhrrr>\n",
            "Perplexity:14633119744.0\n",
            "\n",
            "Epoch:3\n",
            "<snostbaurus>\n",
            "<sksadhucus>\n",
            "<hlbucaurus>\n",
            "<ttraspnusaurur>\n",
            "<lbnuceurts>\n",
            "<maljmures>\n",
            "<kvotaurus>\n",
            "<sjnytmosmrrus>\n",
            "<sbrgsaurup>\n",
            "<cuhictcaurus>\n",
            "<ttreron>\n",
            "<snrttaipapuiuo>\n",
            "<aeriaonaurus>\n",
            "<ivrudhurus>\n",
            "<scwroshunus>\n",
            "Perplexity:4220029780361216.0\n",
            "\n",
            "Epoch:4\n",
            "<euasgsaurul>\n",
            "<cthiclnrus>\n",
            "<bttsolaurus>\n",
            "<qrtaipaurus>\n",
            "<tbcslaaurus>\n",
            "<tcivotaurus>\n",
            "<rriyssauras>\n",
            "<alrasgsaurur>\n",
            "<crnpatas>\n",
            "<pucotrsaurus>\n",
            "<lrsubaurus>\n",
            "<slucesanrus>\n",
            "<hbtanturus>\n",
            "<snbsop>\n",
            "<snostcaurus>\n",
            "Perplexity:60500.1875\n",
            "\n",
            "Epoch:5\n",
            "<sluccsarrus>\n",
            "<hbtchso>\n",
            "<autsmnaurus>\n",
            "<ptubisaurus>\n",
            "<tbasjasaurus>\n",
            "<ahuotasaurus>\n",
            "<jysnosirrus>\n",
            "<scrloca>\n",
            "<laifopaurus>\n",
            "<ouattsaurus>\n",
            "<snostapaurus>\n",
            "<gscbsairus>\n",
            "<gctanxosaurus>\n",
            "<rsnysksaurus>\n",
            "<grcrnrca>\n",
            "Perplexity:805648334848.0\n",
            "\n",
            "Epoch:6\n",
            "<maobthsaurus>\n",
            "<tautrngosaurus>\n",
            "<tahnanaurus>\n",
            "<cpnaolaurus>\n",
            "<juruasaurus>\n",
            "<ixsnosaurus>\n",
            "<scslrcatod>\n",
            "<ctnhasaurus>\n",
            "<sutngsaurus>\n",
            "<suaipaurus>\n",
            "<tcatocaurus>\n",
            "<scivosaurus>\n",
            "<ronytrauris>\n",
            "<alscopaurus>\n",
            "<litmhchurus>\n",
            "Perplexity:350496064.0\n",
            "\n",
            "Epoch:7\n",
            "<bttspaurus>\n",
            "<lrrtaraurus>\n",
            "<euacosaurus>\n",
            "<atantsdurus>\n",
            "<eronytsauris>\n",
            "<aetbtnttrus>\n",
            "<mauenasaurus>\n",
            "<supneosaurus>\n",
            "<tbhnanaurus>\n",
            "<crnapaurus>\n",
            "<ahurrkaurus>\n",
            "<shwtnsaurus>\n",
            "<fscsisaurus>\n",
            "<ctenasaurus>\n",
            "<surijosaurus>\n",
            "Perplexity:107517688.0\n",
            "\n",
            "Epoch:8\n",
            "<tbinanaurus>\n",
            "<btlbolaurus>\n",
            "<jursascurus>\n",
            "<kxtlscurus>\n",
            "<euaomtcaurgs>\n",
            "<senatcaurus>\n",
            "<spnlosaurus>\n",
            "<tanianaurus>\n",
            "<bsibinaurus>\n",
            "<jvntasaurus>\n",
            "<jysnosaurus>\n",
            "<scpltcauris>\n",
            "<smhcodaurus>\n",
            "<tuocsosuurus>\n",
            "<annalmurus>\n",
            "Perplexity:261082710016.0\n",
            "\n",
            "Epoch:9\n",
            "<bupaoesaurus>\n",
            "<wruasssaurus>\n",
            "<soosocaurus>\n",
            "<sltcauras>\n",
            "<senatcaurus>\n",
            "<sonesaurus>\n",
            "<suanicaurus>\n",
            "<agthbonaurus>\n",
            "<hvrudsurus>\n",
            "<snvonosaurus>\n",
            "<scolocauris>\n",
            "<senaucitaurus>\n",
            "<latriwsosaurus>\n",
            "<euathsaurus>\n",
            "<cuemauros>\n",
            "Perplexity:878371.0625\n",
            "\n",
            "Epoch:10\n",
            "<tgstoraurus>\n",
            "<krptaencpaurus>\n",
            "<bslbonaurus>\n",
            "<jvntasaurus>\n",
            "<kyskrpulus>\n",
            "<euashsaurus>\n",
            "<cuenasaurus>\n",
            "<srsnkcurus>\n",
            "<rsuaoraurus>\n",
            "<taksobaurus>\n",
            "<sanysturus>\n",
            "<csrivordtods>\n",
            "<altconiurus>\n",
            "<laupicaurus>\n",
            "<btusonaurus>\n",
            "Perplexity:4086688.75\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL07Mt8_he2O"
      },
      "source": [
        "## Test model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhPGyAdvhd49",
        "outputId": "3faf3d5d-238a-4867-f7fe-69512c393b47"
      },
      "source": [
        "ids = sample(model)\n",
        "print_sample(ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<ronytsaurus>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIS7nyvwgy8p"
      },
      "source": [
        "## Task 1.\n",
        "Rewrite the sampling function so that pangrams (words that contain each character of the alphabet only once)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b3Z6oWSg8W6"
      },
      "source": [
        "# your code here\n",
        "def sample(model):\n",
        "    model.eval()\n",
        "    word_size=0\n",
        "    newline_idx = trn_ds.ch_to_idx['>']\n",
        "    with torch.no_grad():\n",
        "        h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
        "        x = h_prev.new_zeros([1, trn_ds.vocab_size])\n",
        "        start_char_idx = trn_ds.ch_to_idx['<']\n",
        "        indices = [start_char_idx]\n",
        "        x[0, start_char_idx] = 1\n",
        "        predicted_char_idx = start_char_idx\n",
        "        \n",
        "        while predicted_char_idx != newline_idx and word_size != 50:\n",
        "            h_prev, y_pred = model(h_prev, x)\n",
        "            y_softmax_scores = torch.softmax(y_pred, dim=1)\n",
        "            \n",
        "            idx = indices[-1]\n",
        "            while idx in indices:\n",
        "                np.random.seed(np.random.randint(1, 5000))\n",
        "                idx = np.random.choice(np.arange(trn_ds.vocab_size), p=y_softmax_scores.cpu().numpy().ravel())\n",
        "            indices.append(idx)\n",
        "            \n",
        "            x = (y_pred == y_pred.max(1)[0]).float()\n",
        "            \n",
        "            predicted_char_idx = idx\n",
        "            \n",
        "            word_size += 1\n",
        "        \n",
        "        if word_size == 50:\n",
        "            indices.append(newline_idx)\n",
        "    return indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAfFlA53kjM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae2e961-8805-4da8-c511-07eccca50bc6"
      },
      "source": [
        "print_sample(sample(model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tarkedlunis>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgrQxISAgzP5"
      },
      "source": [
        "## Task 2.\n",
        "Rewrite the sampling function so that is it is possible to change the sampling temperature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yww3y4Y6jHF3"
      },
      "source": [
        "def equalize_probs_sqrt(in_vector):\n",
        "    out_vector = np.zeros_like(in_vector)\n",
        "    for i, el in enumerate(in_vector):\n",
        "        out_vector[i] = np.math.sqrt(el)\n",
        "\n",
        "    return out_vector / sum(out_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzEDyiVbg_MB"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b67Qs9XCgzhW"
      },
      "source": [
        "##Task 3.\n",
        "Implement the beam search for sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLivB9yEgbYz"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}