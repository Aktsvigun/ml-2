{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6ccN1AlFhNo"
      },
      "source": [
        "## Text classification using CNN\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1U3vnZeD8aiDg5Gh-SjnEyJyfrTHSRTkB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ8_zAI8FhNp"
      },
      "source": [
        "In this seminar we are going to build a CNN sentiment classifier using the IMDB review dataset. \n",
        "\n",
        "Materials source: https://github.com/bentrevett/pytorch-sentiment-analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHKLq9hEFhNr"
      },
      "source": [
        "Assuming PyTorch is already installed, let's install additional modules and load the model for tokenization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgGxeuQjG9NI"
      },
      "source": [
        "# !pip3 install https://download.pytorch.org/whl/cpu/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kiZro9eG-bG",
        "outputId": "91262cd0-c8cb-4799-9626-d7a9bd6f6a17"
      },
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-WRgs-VFhNt",
        "outputId": "352fa11f-22c5-47a8-d3ca-09646b446ae0"
      },
      "source": [
        "!pip install torchtext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.19.5)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.9.0+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB7uZ5L7FhN3"
      },
      "source": [
        "# !pip3 install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLGeXcUjFhN8"
      },
      "source": [
        "#!python3.6 -m spacy download en\n",
        "# !python3 -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaPk16PLmgnw"
      },
      "source": [
        "# import spacy\n",
        "#import en\n",
        "# en_nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zclld6vmaH2"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4Yq-LOGmoOn",
        "outputId": "20013a5d-0f53-4823-b729-5cd07b253043"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.9.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZlniJuAmp8F"
      },
      "source": [
        "SEED = 0\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BObMLa5mvQYM"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLGAJBxfFhN_"
      },
      "source": [
        "Let's load the dataset and get a sample from it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7w84GaXumr0q",
        "outputId": "01775155-51cd-45aa-dbd3-6ee7e7bce6b7"
      },
      "source": [
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=17uuANm7Q1CunXHfTaF7IRY9Vy7qPl5_L' -O imdb.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-11-08 17:01:24--  https://drive.google.com/uc?export=download&id=17uuANm7Q1CunXHfTaF7IRY9Vy7qPl5_L\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.69.101, 74.125.69.139, 74.125.69.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.69.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/5q321qft822gj3qbrjvgpsdl6fnt49nt/1636390875000/13414369628864094336/*/17uuANm7Q1CunXHfTaF7IRY9Vy7qPl5_L?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-11-08 17:01:31--  https://doc-0c-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/5q321qft822gj3qbrjvgpsdl6fnt49nt/1636390875000/13414369628864094336/*/17uuANm7Q1CunXHfTaF7IRY9Vy7qPl5_L?e=download\n",
            "Resolving doc-0c-44-docs.googleusercontent.com (doc-0c-44-docs.googleusercontent.com)... 209.85.200.132, 2607:f8b0:4001:c16::84\n",
            "Connecting to doc-0c-44-docs.googleusercontent.com (doc-0c-44-docs.googleusercontent.com)|209.85.200.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 66212309 (63M) [text/csv]\n",
            "Saving to: ‘imdb.csv’\n",
            "\n",
            "imdb.csv            100%[===================>]  63.14M   131MB/s    in 0.5s    \n",
            "\n",
            "2021-11-08 17:01:32 (131 MB/s) - ‘imdb.csv’ saved [66212309/66212309]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG25VDn1vdNn"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "xsHAXknhFhOA",
        "outputId": "4582c231-8048-49f1-ffc9-f1cbeabb8327"
      },
      "source": [
        "df = pd.read_csv('imdb.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WAUUMcixIRP"
      },
      "source": [
        "### Write data to compatable structures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT8eYs4dxWW1"
      },
      "source": [
        "from torchtext import data\n",
        "# data.Field is obsolete now\n",
        "from torchtext.legacy import data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck6B9K1ayX7A"
      },
      "source": [
        "# Field and LabelField classes are responsible for the way data will be stored and processed\n",
        "TEXT = data.Field(tokenize='spacy') # we'll use spacy for tokenization here\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "ds = data.TabularDataset(\n",
        "  path='imdb.csv', format='csv',\n",
        "  skip_header=True,\n",
        "  fields=[('text', TEXT),\n",
        "        ('label', LABEL)]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdpfQN9qOu0p"
      },
      "source": [
        "ds - dataset - iterates through our texts & labels.\n",
        "\n",
        "**NB**: original column names don't matter since we pass column names to the `fields` argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8-CSOf1xHuV",
        "outputId": "78cb13df-8143-4e01-cba5-f2c83b5dcecd"
      },
      "source": [
        "next(ds.text)[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['One',\n",
              " 'of',\n",
              " 'the',\n",
              " 'other',\n",
              " 'reviewers',\n",
              " 'has',\n",
              " 'mentioned',\n",
              " 'that',\n",
              " 'after',\n",
              " 'watching']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e9h0nnsi0Q0-",
        "outputId": "0ae72213-bf8c-4aea-b781-11760a555d7f"
      },
      "source": [
        "next(ds.label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'positive'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECxpxH2m9VY8"
      },
      "source": [
        "Build the dictionary and load embeddings.\n",
        "\n",
        "Taking into account the fact that there are 100K unique words in the collection, and the vectors are big, we will truncate the collection down to 25K words, and set the unk (unknown) token for all the other words.\n",
        "\n",
        "Torchtext has a repository with some of the vocabulary embeddings for English. `vectors =\" glove.6B.100d \"` means that in addition to building an index of words in the corpus, we will download and save the glove vectors from this repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K_LTwEn78n5",
        "outputId": "1d4e1774-781c-470f-edc4-8b5be0d315ad"
      },
      "source": [
        "TEXT.build_vocab(ds, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "LABEL.build_vocab(ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:41, 5.33MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:20<00:00, 19310.88it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9omo_uS9P7V",
        "outputId": "8356f903-710f-4f39-e094-87377e96ddf3"
      },
      "source": [
        "# itos == i to s == index to string\n",
        "print(TEXT.vocab.itos[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is', 'in', 'I', 'it', 'that', '\"', \"'s\", 'this', '-', '/><br', 'was']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNFCBfJZa98r",
        "outputId": "2990cd4e-0683-4b66-f988-c75871511541"
      },
      "source": [
        "TEXT.vocab.itos[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<unk>',\n",
              " '<pad>',\n",
              " 'the',\n",
              " ',',\n",
              " '.',\n",
              " 'a',\n",
              " 'and',\n",
              " 'of',\n",
              " 'to',\n",
              " 'is',\n",
              " 'in',\n",
              " 'I',\n",
              " 'it',\n",
              " 'that',\n",
              " '\"',\n",
              " \"'s\",\n",
              " 'this',\n",
              " '-',\n",
              " '/><br',\n",
              " 'was']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZRu4ykFpz_q",
        "outputId": "706852f4-d836-4026-9b0a-a82fdfc3712e"
      },
      "source": [
        "# stoi == s to i == string to index\n",
        "TEXT.vocab.stoi[42]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJl0ZOPBElSE"
      },
      "source": [
        "Let's break down our dataset into training, validation (for parameters evaluation) and test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nuZQIq1FhOl"
      },
      "source": [
        "train, val = ds.split() # default split is 0.7\n",
        "val, test = val.split(split_ratio=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4_u1nkyS740",
        "outputId": "db5d9766-dab6-4b63-f917-7b83d024f8d1"
      },
      "source": [
        "print(len(train))\n",
        "print(len(val))\n",
        "print(len(test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35000\n",
            "7500\n",
            "7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT-lY0stTbWo"
      },
      "source": [
        "Now let's create batch iterators:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIJTX3ypTbrK"
      },
      "source": [
        "BATCH_SIZE  = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train, val, test), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    sort=True,\n",
        "    sort_key=lambda x: len(x.text), # sort texts by length so that there are sentences with the same length next to each other and less padding is added\n",
        "    repeat=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JtvOuVbT3i7"
      },
      "source": [
        "Let's take a look inside the batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQuNMEB5Tgmz"
      },
      "source": [
        "for i, batch in enumerate(test_iterator):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kE9EDfGT_Gr",
        "outputId": "43d4e739-167f-4ed3-e423-2e55d5adf518"
      },
      "source": [
        "batch.fields"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['text', 'label'])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Kvf2Hf2UEtR",
        "outputId": "01acae24-2c93-43a7-d0bb-469d50a45a48"
      },
      "source": [
        "batch.batch_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1oLPN8-TuA_",
        "outputId": "bcc0cfb5-dd6c-4ee0-fe57-5f7202849ae8"
      },
      "source": [
        "batch.text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[3100,  170,  596,  ...,   66,   66,  825],\n",
              "        [  10,  287,   34,  ...,    9,   21,  140],\n",
              "        [   2,  145, 6769,  ...,    3,   19,    3],\n",
              "        ...,\n",
              "        [   2,    1,    1,  ...,    1,    1,    1],\n",
              "        [ 235,    1,    1,  ...,    1,    1,    1],\n",
              "        [   4,    1,    1,  ...,    1,    1,    1]])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_TjGTb0UHDE",
        "outputId": "94f45530-09ab-4949-fee5-6cbae37ca4f1"
      },
      "source": [
        "batch.label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7i8JbEnULFC"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTPx1IJ2SlXf"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRr8D0t3Sl1a"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHEDOjrAFhP5"
      },
      "source": [
        "We will use nn.Conv2d to create a convolutional layer. in our case `in_channels` is one (text), `out_channels` is the number of filters and the size of the kernels of all filters. Each filter will have a dimension [n x embedding dimension], where n is the size of the n-gram being processed.\n",
        "\n",
        "It is important that the sentences were at least as long as the size of the largest filter used (this is not a problem our case since the dataset doesn't contain texts consisting of five or less words)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb5g3czaFhP6"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout_proba):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0], embedding_dim))\n",
        "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1], embedding_dim))\n",
        "        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2], embedding_dim))\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_proba)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #x = [sent len, batch size]\n",
        "        x = x.permute(1, 0)\n",
        "                \n",
        "        #x = [batch size, sent len]\n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
        "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
        "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
        "            \n",
        "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
        "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
        "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
        "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "        return self.fc(cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_xPva9zFhP8"
      },
      "source": [
        "Now we can only use three different filters, but we can create more. In general, you can use `nn.ModuleList` to create layers as a list and make filters based on the number of elements in filter_sizes. [(Like here).](Https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FenASHjKUb3p"
      },
      "source": [
        "### Supplementary functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf2viKyOE5FF"
      },
      "source": [
        "Let us describe the function for accuracy calculation, as well as the functions for train and evaluation of the network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQo4PuPzFhPE"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(F.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jepSRNlkFhPI"
      },
      "source": [
        "def train_func(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text.cuda()).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions.float(), batch.label.float().cuda())\n",
        "        acc = binary_accuracy(predictions.float(), batch.label.float().cuda())\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss\n",
        "        epoch_acc += acc\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb0KPBl8FhPL"
      },
      "source": [
        "def evaluate_func(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            predictions = model(batch.text.cuda()).squeeze(1)\n",
        "\n",
        "            loss = criterion(predictions.float(), batch.label.float().cuda())\n",
        "            acc = binary_accuracy(predictions.float(), batch.label.float().cuda())\n",
        "\n",
        "            epoch_loss += loss\n",
        "            epoch_acc += acc\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6CzZJecUpN7"
      },
      "source": [
        "### Training preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q_Yvs_gFhQB"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3,4,5]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT_PROBA = 0.5\n",
        "\n",
        "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT_PROBA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-5mPhI4U0nk",
        "outputId": "c510008a-6e40-493f-f6de-946e5b6a7b01"
      },
      "source": [
        "model # let's look at the model again"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (embedding): Embedding(25002, 100)\n",
              "  (conv_0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
              "  (conv_1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
              "  (conv_2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
              "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVfQU3wUU4BY"
      },
      "source": [
        "Copy downloaded word embeddings to the parameters of the `Embedding` layer, so that you don't need to train it from the very beginning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18Vk11CaZG-u",
        "outputId": "f3b87c53-9c8e-4506-b37a-bbf26756d975"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [ 0.4413,  0.3325,  0.1120,  ..., -0.0686,  0.4374,  0.8717],\n",
              "        [ 0.1177,  0.1141,  0.2218,  ..., -1.0694,  0.4712, -0.7554],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Yqwwc5rBc5L"
      },
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "np-BTnydFhQF"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters()) # we have given all parameters to the optimizer, so embeddigs will also be fitted\n",
        "criterion = nn.BCEWithLogitsLoss() # binary cross-entropy with logits\n",
        "\n",
        "model = model.cuda() # we will train on gpu! =)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmWIaqIbFhQF"
      },
      "source": [
        "### Training!\n",
        "\n",
        "Using the previously defined functions, let's start training with the Adam optimizer and evaluate the quality on validation and test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XZC7S33pFhQH",
        "outputId": "02100e68-c3c0-4108-800b-e93850ab4568"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train_func(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate_func(model, valid_iterator, criterion)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01, Train Loss: 0.400, Train Acc: 81.62%, Val. Loss: 0.299, Val. Acc: 86.99%\n",
            "Epoch: 02, Train Loss: 0.248, Train Acc: 90.02%, Val. Loss: 0.265, Val. Acc: 89.08%\n",
            "Epoch: 03, Train Loss: 0.175, Train Acc: 93.34%, Val. Loss: 0.271, Val. Acc: 88.87%\n",
            "Epoch: 04, Train Loss: 0.120, Train Acc: 95.61%, Val. Loss: 0.309, Val. Acc: 88.33%\n",
            "Epoch: 05, Train Loss: 0.080, Train Acc: 97.34%, Val. Loss: 0.315, Val. Acc: 89.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bXQYQCLCFhQJ",
        "outputId": "22b6ef85-73bf-4b93-cd73-8f1544ae2239"
      },
      "source": [
        "test_loss , test_acc = evaluate_func(model, test_iterator, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.302, Test Acc: 89.80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm5CmdjtY6Zj"
      },
      "source": [
        "#### Exercise 1: How did embeddings change?\n",
        "\n",
        "Let's check if there have been any significant changes in the relationship between words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xbAon9WMY61-",
        "outputId": "220d413b-1809-4948-fc83-83a187b385b0"
      },
      "source": [
        "TEXT.vocab.vectors # old embeddings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [ 0.4413,  0.3325,  0.1120,  ..., -0.0686,  0.4374,  0.8717],\n",
              "        [ 0.1177,  0.1141,  0.2218,  ..., -1.0694,  0.4712, -0.7554],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Oej9zvPYarQK",
        "outputId": "f428911c-3fbb-4651-d68b-312b748f6452"
      },
      "source": [
        "model.embedding.weight.data # new emdeddings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.1253,  0.0091,  0.0669,  ..., -0.0644,  0.3358, -0.0415],\n",
              "        [ 0.0163, -0.0274, -0.0355,  ...,  0.0492,  0.0890,  0.0365],\n",
              "        [ 0.0577, -0.2450,  0.6177,  ..., -0.0722,  0.7610,  0.2088],\n",
              "        ...,\n",
              "        [ 0.3600,  0.3053,  0.0656,  ..., -0.1083,  0.3947,  0.9758],\n",
              "        [ 0.1465,  0.1240,  0.2414,  ..., -1.0659,  0.4765, -0.7625],\n",
              "        [ 0.0943, -0.0828, -0.0432,  ...,  0.1776, -0.1210, -0.1947]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k8mHfM5MbhSx"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qcFo6zIfbVvL"
      },
      "source": [
        "i1, i2 = TEXT.vocab.stoi['perfect'], TEXT.vocab.stoi['awful']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "17bFXBwhbliQ",
        "outputId": "43830ee3-0aea-44eb-d124-f78b06a36a68"
      },
      "source": [
        "cosine_similarity([\n",
        "  TEXT.vocab.vectors[i1].cpu().numpy(),\n",
        "  TEXT.vocab.vectors[i2].cpu().numpy()\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.9999999, 0.5248411],\n",
              "       [0.5248411, 0.9999996]], dtype=float32)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YUFaea6bbw8f",
        "outputId": "289b0dbb-2037-4cd2-e99d-f0477a369a89"
      },
      "source": [
        "cosine_similarity([\n",
        "  model.embedding.weight.data[i1].cpu().numpy(),\n",
        "  model.embedding.weight.data[i2].cpu().numpy()\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.0000001, 0.3984493],\n",
              "       [0.3984493, 1.       ]], dtype=float32)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0XE_D_MWE22"
      },
      "source": [
        "\"perfect\" and \"awful\" are further from each other now.\n",
        "\n",
        "**Task**: Look at the other changes and try to explain them. You can make a visualization using t-sne for clarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aVvpv9w-WFOc"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICox-i-WYYPq"
      },
      "source": [
        "#### Excersise 2: nn.ModuleList\n",
        "\n",
        "You can easily define as many different convolutions as you like using nn.ModuleList! Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eKCFgWKdXeyO",
        "outputId": "8b05388b-4d0f-476b-9ef5-cf651867e2f2"
      },
      "source": [
        "|class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "                \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv2d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = (fs, embedding_dim)) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "                \n",
        "        #text = [batch size, sent len]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        \n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "            \n",
        "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "                \n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        \n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "            "
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-47-a5d587e8d4d3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    |class CNN(nn.Module):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSb06TT3cwO9"
      },
      "source": [
        "**Task**: experiment with the number and size of the bundles. Which works best?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eO5TShOrc5OV"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGNUN9TTYnxO"
      },
      "source": [
        "#### Exercise 3: Another preprocessing\n",
        "\n",
        "We used `data.Field (tokenize = 'spacy')` when loading data.\n",
        "Let's try to replace the `spacy` tokenizer with our own function, which additionally cleans data from garbage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YkmL4MxZZ1N0"
      },
      "source": [
        "# пример мусора\n",
        "ds.examples[0].text[25:40]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRV729TtZfxC"
      },
      "source": [
        "Preprocessing (from the last workshop):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bnDJ5sFwZfxD"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iL2EXExRZfxF"
      },
      "source": [
        "def review_to_wordlist(review):\n",
        "    # remove links\n",
        "    review = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", review)\n",
        "    # get the text\n",
        "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
        "    # keep only word symbols\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "    # convert words to lowercase and split into words by space character\n",
        "    return review_text.lower().split() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXbasRI5dBfM"
      },
      "source": [
        "**Task**: Try to train the model using a different preprocessing. Has it gotten better? What if we remove the stop words?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L4PiG-hIZfxH"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sfuamkw28rgp"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89zDRG-jIdNB"
      },
      "source": [
        "In our example, the data was balanced, but how to deal with unbalanced data?\n",
        "\n",
        "Consider the problem of recognizing the sentiment of tweets taken from the [Twitter Sentimental Analysis challenge](https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/).\n",
        "\n",
        "Presentation source: https://github.com/mabusalah/Resampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLKzWFu-QWAm"
      },
      "source": [
        "Downoad the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5_WpbqWhk-a5"
      },
      "source": [
        "!wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=1Jjuk23nMTQkfA3-3_HpevXGeupav7QLz\" -O train.csv\n",
        "!wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=11FugxTRrdKqkDE_3KlfCDWRn_rbR6VxM\" -O test.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ILRsSOuWJGFP"
      },
      "source": [
        "import pandas as pd\n",
        "test = pd.read_csv('test.csv')\n",
        "print(\"Test Set:\"% test.columns, test.shape, len(test))\n",
        "train = pd.read_csv('train.csv')\n",
        "print(\"Training Set:\"% train.columns, train.shape, len(train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sBtAsQDNJPXX"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XV-4lGQVJRc2"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xarXsO4DJU8Y"
      },
      "source": [
        "Let us see the percentage of the total samples in positive and negative examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eT7Rz3MNJTVP"
      },
      "source": [
        "print(\"Positive: \", train.label.value_counts()[0]/len(train)*100,\"%\")\n",
        "print(\"Negative: \", train.label.value_counts()[1]/len(train)*100,\"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB6Jaz-mJ1zR"
      },
      "source": [
        "93% vs. 7% - the data is definitely unbalanced, which, in turn, negatively affects the accuracy of the prediction.\n",
        "First, let's work with the initial data and evaluate the classification accuracy. Let's start with data preprocessing: remove numbers, html / xml tags, special characters from tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FCmZHO1kJfpP"
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup #handling html/xml tags\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter=PorterStemmer()\n",
        "tok = WordPunctTokenizer()\n",
        "pat1 = r'@[A-Za-z0-9]+'\n",
        "pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "combined_pat = r'|'.join((pat1, pat2))\n",
        "\n",
        "def tweet_cleaner(text):\n",
        "    soup = BeautifulSoup(text, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    stripped = re.sub(combined_pat, '', souped)\n",
        "    try:\n",
        "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "        clean = stripped\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
        "    lower_case = letters_only.lower()\n",
        "\n",
        "    words = tok.tokenize(lower_case)\n",
        "    \n",
        "    stem_sentence=[]\n",
        "    for word in words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    words=\"\".join(stem_sentence).strip()\n",
        "    return words\n",
        "\n",
        "nums = [0,len(train)]\n",
        "clean_tweet_texts = []\n",
        "for i in range(nums[0],nums[1]):\n",
        "    clean_tweet_texts.append(tweet_cleaner(train['tweet'][i]))\n",
        "    \n",
        "nums = [0,len(test)]\n",
        "test_tweet_texts = []\n",
        "\n",
        "for i in range(nums[0],nums[1]):\n",
        "    test_tweet_texts.append(tweet_cleaner(test['tweet'][i])) \n",
        "    \n",
        "train_clean = pd.DataFrame(clean_tweet_texts,columns=['tweet'])\n",
        "train_clean['label'] = train.label\n",
        "train_clean['id'] = train.id\n",
        "test_clean = pd.DataFrame(test_tweet_texts,columns=['tweet'])\n",
        "test_clean['id'] = test.id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkQ5fDVUM9EU"
      },
      "source": [
        "Let's divide the data into training and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d23SW3-tMdKk"
      },
      "source": [
        "from sklearn import model_selection, preprocessing, metrics, linear_model, svm\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_clean['tweet'],train_clean['label'])\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfj1ZjQANVSK"
      },
      "source": [
        "Let's calculate TF-IDF weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RNw9U1WnNKfq"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=100000)\n",
        "tfidf_vect.fit(train_clean['tweet'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8TxxultNlOq"
      },
      "source": [
        "Accuracy metric works well only for balanced datasets, so we will use the F1 measure to evaluate the results of the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cx29ZstgNXqb"
      },
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "\n",
        "    predictions = classifier.predict(feature_vector_valid)    \n",
        "\n",
        "    return metrics.f1_score(valid_y,predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YEjiEUDNrLj"
      },
      "source": [
        "First, let's train log regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BG3DifE-Nn-I"
      },
      "source": [
        "accuracyORIGINAL = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression Baseline, WordLevel TFIDF: \", accuracyORIGINAL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIaiuRviW5pS"
      },
      "source": [
        "Try using word count vectorizer for feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Vsr0wT4NW2LW"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0baBnaYN7a0"
      },
      "source": [
        "As you can see, we obtain poor result.\n",
        "\n",
        "What can be done with the data?\n",
        "\n",
        "It would be nice to somehow increase the number of negative examples, or reduce the number of positive ones. There are various data augmentation techniques for this. Python has imblearn library (imbalanced-learn) for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "F6zNqmnUN1WB"
      },
      "source": [
        "from imblearn.over_sampling import BorderlineSMOTE, SMOTE, ADASYN, SMOTENC, RandomOverSampler\n",
        "from imblearn.under_sampling import (RandomUnderSampler, \n",
        "                                    NearMiss, \n",
        "                                    InstanceHardnessThreshold,\n",
        "                                    CondensedNearestNeighbour,\n",
        "                                    EditedNearestNeighbours,\n",
        "                                    RepeatedEditedNearestNeighbours,\n",
        "                                    AllKNN,\n",
        "                                    NeighbourhoodCleaningRule,\n",
        "                                    OneSidedSelection,\n",
        "                                    TomekLinks)\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.pipeline import make_pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efoIi6euYMQs"
      },
      "source": [
        "Consider using under-sampling, over-sampling and their combination for augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqhD2dodYwJl"
      },
      "source": [
        "**Under-sampling** balances the data by reducing the size of the prevailing class.\n",
        "It is reasonable to use this method when the amount of data is large enough, otherwise there is a risk of being left without training examples at all.\n",
        "\n",
        "So, the logic of the action is quite simple: we just randomly remove unnecessary instances from the prevailing class.\n",
        "\n",
        "Since in our example only 7% of all tweets are negative, balancing a positive set with this 7% is unlikely to provide a good result.\n",
        "\n",
        "Let's try ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AQVfIvf0aTrp"
      },
      "source": [
        "rus = RandomUnderSampler(random_state=0, replacement=True)\n",
        "rus_xtrain_tfidf, rus_train_y = rus.fit_sample(xtrain_tfidf, train_y)\n",
        "accuracyrus = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),rus_xtrain_tfidf, rus_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regressio RUS, WordLevel TFIDF: \", accuracyrus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnwqxfGeaa9R"
      },
      "source": [
        "Indeed, things only got worse.\n",
        "\n",
        "Let's try other **under-sampling** algorithms.\n",
        "\n",
        "For example, **NearMiss**. This algorithm chooses which instances to keep in the prevailing class based on some heuristics. There are three variants of this algorithm:\n",
        "\n",
        "**NearMiss-1** leaves those instances from the prevailing class for which the average distance to * k * nearest neighbors from the minority class will be the smallest.\n",
        "\n",
        "**NearMiss-2** leaves those instances from the prevailing class for which the average distance to * k * the farthest neighbors from the minority class will be the smallest.\n",
        "\n",
        "**NearMiss-3** consists of two steps: first, for each instance, * k * nearest neighbors from the prevailing class are selected from the minority class, then, from the larger class, those instances are selected for which the average distance to * k * nearest neighbors is maximum ...\n",
        "\n",
        "![](https://glemaitre.github.io/imbalanced-learn/_images/sphx_glr_plot_nearmiss_001.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QSlgmHmOd9tU"
      },
      "source": [
        "for sampler in (NearMiss(version=1),NearMiss(version=2),NearMiss(version=3)):\n",
        "    nm_xtrain_tfidf, nm_train_y = sampler.fit_sample(xtrain_tfidf, train_y)\n",
        "    accuracysm = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),nm_xtrain_tfidf, nm_train_y, xvalid_tfidf)\n",
        "    print (\"Logistic regression NearMiss(version= {0}), WordLevel TFIDF: \".format(sampler.version), accuracysm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9nNg2irexcc"
      },
      "source": [
        "**Edited Nearest Neighbor (ENN)**\n",
        "\n",
        "ENN removes an element from a larger class if its nearest neighbor has a class other than its own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WFjd6PzjfWEU"
      },
      "source": [
        "enn_xtrain_tfidf, enn_train_y = EditedNearestNeighbours().fit_sample(xtrain_tfidf, train_y)\n",
        "accuracy = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),enn_xtrain_tfidf, enn_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression {0}, WordLevel TFIDF: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VcHHV4jgZ50"
      },
      "source": [
        "As you can see, applying the **Under-sampling** technique does not generate new data, unlike the **Over-sampling**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CHYj7jW7z5q"
      },
      "source": [
        "# Over-sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODhcR4G672lq"
      },
      "source": [
        "So, when there is not enough data or the number of instances in a minority class is very small, **Over-sampling** is applied.\n",
        "\n",
        "With this technique, data balancing occurs by increasing the number of instances in the minority class. New elements are generated by: repetition, bootstrapping, **SMOTE** (Synthetic Minority Over-Sampling Technique) or **ADASYN** (Adaptive synthetic sampling)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea9aYY4mxsqo"
      },
      "source": [
        "**Random Over-sampling**: randomly duplicates some elements from the minority class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t-6Ivm0ZOCOt"
      },
      "source": [
        "#Random Over Sampling\n",
        "ros = RandomOverSampler(random_state=777)\n",
        "ros_xtrain_tfidf, ros_train_y = ros.fit_sample(xtrain_tfidf, train_y)\n",
        "accuracyROS = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),ros_xtrain_tfidf, ros_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression ROS, WordLevel TFIDF: \", accuracyROS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXp1gpdDz5p5"
      },
      "source": [
        "**SMOTE Over-sampling**\n",
        "\n",
        "The SMOTE algorithm is based on the idea of ​​generating a number of artificial examples that would be “similar” to those in the minority class, but would not duplicate them.\n",
        "\n",
        "To create a new record, find the difference $d=X_b-X_a$, where $X_b,X_a$ - vectors of features of \"neighboring\" examples $a$ and $b$ from the minority class.\n",
        "\n",
        "They are found using the nearest neighbor algorithm (*KNN*). In this case, it is necessary and sufficient for the $b$ example to obtain a set of $k$ neighbors, from which the record $a$ will be selected in the future. The rest of the steps of the *KNN* algorithm are not required.\n",
        "\n",
        "Then, from $d$, by multiplying each of its elements by a random number in the interval (0, 1), $\\hat{d}$ is obtained. The feature vector of the new example is calculated by adding $X_a$ and $\\hat{d}$.\n",
        "\n",
        "The **SMOTE** algorithm allows you to specify the number of records that must be artificially generated. The degree of similarity between the examples $ a $ and $ b $ can be adjusted by changing the value of $ k $ (the number of nearest neighbors).\n",
        "\n",
        "![](https://hsto.org/getpro/habr/post_images/c57/e7e/f4f/c57e7ef4f8711ad2eda881651a027867.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HWzqbIWsOEg1"
      },
      "source": [
        "sm = SMOTE(random_state=777, ratio = 1.0)\n",
        "sm_xtrain_tfidf, sm_train_y = sm.fit_sample(xtrain_tfidf, train_y)\n",
        "accuracySMOTE = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),sm_xtrain_tfidf, sm_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression SMOTE, WordLevel TFIDF: \", accuracySMOTE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gpJkvdb10Sg"
      },
      "source": [
        "So, compared to **Random Over-sampling**, the difference is small.\n",
        "\n",
        "Check **Random Over-sampling** and **SMOTE Over-sampling** results for real test data (*test_clean*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7V-VoCAO7qw0"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Jz0o7k82YoX"
      },
      "source": [
        "The following algorithm is **ASMO: Adaptive synthetic minority oversampling**.\n",
        "\n",
        "Generate artificial records within individual clusters based on all classes. For each example of a minority class, the m nearest neighbors are found, and based on them (as in SMOTE) new records are created.\n",
        "\n",
        "1. If for each $i$ th example of a minority class from $k$ nearest neighbors $g$ ($g\\leq k$) belongs to the majority class, then the dataset is considered \"scattered\". In this case, the **ASMO** algorithm is used, otherwise **SMOTE** is used (as a rule, $g$ is set equal to 20).\n",
        "2. Using only minority class examples, select several clusters (for example, using the $k$ -means algorithm).\n",
        "3. Generate artificial records within individual clusters based on all classes. For each example of a minority class, the m nearest neighbors are found, and based on them (as in **SMOTE**) new records are created.\n",
        "\n",
        "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQdTzjHBZ_9At5GIDRpF2AAw9hU1jzcVE5uwA&usqp=CAU)\n",
        "\n",
        "This modification of the **SMOTE** algorithm makes it more adaptable to different datasets with unbalanced classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mClRDnFM1weo"
      },
      "source": [
        "ad = ADASYN(random_state=777, ratio = 1.0)\n",
        "ad_xtrain_tfidf, ad_train_y = ad.fit_sample(xtrain_tfidf, train_y)\n",
        "accuracyADASYN = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),ad_xtrain_tfidf, ad_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression ADASYN, WordLevel TFIDF: \", accuracyADASYN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHRcvIRP4fx7"
      },
      "source": [
        "Let's check it again with real test examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YTzMbjvnRyFD"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br9FjH077ix0"
      },
      "source": [
        "# Combination of **Under-** and **Over-sampling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-RCI42f42Hi"
      },
      "source": [
        "Possible combinations can be implemented using *imblearn*:\n",
        "\n",
        "1. **SMOTE** + **ENN**\n",
        "2. **SMOTE** + **Tomek Link Removal** (A pair of two nearest neighbors that belong to different classes is called *Tomek link*. Under-sampling is to remove all such elements from the majority class)\n",
        "\n",
        "More details: https://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.combine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3pg1YBvT4-xP"
      },
      "source": [
        "se = SMOTEENN(random_state=42)\n",
        "se_xtrain_tfidf, se_train_y = se.fit_sample(xtrain_tfidf, train_y)\n",
        "accuracy = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),se_xtrain_tfidf, se_train_y, xvalid_tfidf)\n",
        "print (\"Logistic regression SMOTEENN: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gd9dCEd5aDs"
      },
      "source": [
        "The first method did not work well. Evaluate the results of the second approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bkZofkXo5RG2"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}