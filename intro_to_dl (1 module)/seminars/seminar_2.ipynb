{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NPyI6bkceMr"
   },
   "source": [
    "*Made by Artem Vazhentsev (AIRI)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkzDQSGlYiI8"
   },
   "source": [
    "# Special Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqFsOsI5a7XB"
   },
   "source": [
    "In this seminar, we will use the same neural network and task as in the previous seminar. \n",
    "\n",
    "First of all, train the model and add one linear layer, which we will modify further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RTlTPrk5rvJj"
   },
   "outputs": [],
   "source": [
    "# %load nn_imports.py\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eUTPTJVTr9wD"
   },
   "outputs": [],
   "source": [
    "!wget  -O 'housing_data.csv' -q 'https://www.dropbox.com/s/6dxq90t0prn2vaw/_train_sem2.csv?dl=0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "oLkp-UxPsDDx",
    "outputId": "2b181d63-b93c-4bee-d1a9-130b11742c61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('housing_data.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "Jfdy0amasEoZ",
    "outputId": "47004d6b-23d7-416a-ed2d-69a9f353c935"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978</td>\n",
       "      <td>...</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655</td>\n",
       "      <td>...</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0   1          60         65.0     8450            7            5       2003   \n",
       "1   2          20         80.0     9600            6            8       1976   \n",
       "2   3          60         68.0    11250            7            5       2001   \n",
       "3   4          70         60.0     9550            7            5       1915   \n",
       "4   5          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  WoodDeckSF  OpenPorchSF  \\\n",
       "0          2003       196.0         706  ...           0           61   \n",
       "1          1976         0.0         978  ...         298            0   \n",
       "2          2002       162.0         486  ...           0           42   \n",
       "3          1970         0.0         216  ...           0           35   \n",
       "4          2000       350.0         655  ...         192           84   \n",
       "\n",
       "   EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  MoSold  YrSold  \\\n",
       "0              0          0            0         0        0       2    2008   \n",
       "1              0          0            0         0        0       5    2007   \n",
       "2              0          0            0         0        0       9    2008   \n",
       "3            272          0            0         0        0       2    2006   \n",
       "4              0          0            0         0        0      12    2008   \n",
       "\n",
       "   SalePrice  \n",
       "0     208500  \n",
       "1     181500  \n",
       "2     223500  \n",
       "3     140000  \n",
       "4     250000  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.select_dtypes(['int64', 'float64'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cLYs8eBBsFBZ"
   },
   "outputs": [],
   "source": [
    "X = data.drop(columns=['Id', 'SalePrice']).fillna(data.mean()).values.astype(np.float32)\n",
    "y = data.SalePrice.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9055vQwTsFDj"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PL38sM0dsFFo"
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nE6hRasssFH2"
   },
   "outputs": [],
   "source": [
    "ss2 = StandardScaler()\n",
    "y_train = ss2.fit_transform(y_train[:, None]).reshape(-1)\n",
    "y_test = ss2.transform(y_test[:, None]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "F8hR7jd3sFJu"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "SEED = 42 # random seed for reproducibility\n",
    "LR = 3e-2 # learning rate, controls the speed of the training\n",
    "WEIGHT_DECAY = 1e-3 # lambda for L2 reg. ()\n",
    "NUM_EPOCHS = 5 # num training epochs (how many times each instance will be processed)\n",
    "GAMMA = 0.9995 # learning rate scheduler parameter\n",
    "BATCH_SIZE = 32 # training batch size\n",
    "EVAL_BATCH_SIZE = 300 # evaluation batch size.\n",
    "DEVICE = 'cpu' #'cuda' # device to make the calculations on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8FLoFcCtsZNq"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rob9aK39sZ8N"
   },
   "outputs": [],
   "source": [
    "# Initialize the DataObject, which must return an element (features vector x and target value y)\n",
    "# for a given idx. This class must also have a length atribute\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__() # to initialize the parent class\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = len(X)\n",
    "\n",
    "    def __len__(self): # We use __func__ for implementing in-built python functions\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ga3ZJQxysZ-R"
   },
   "outputs": [],
   "source": [
    "# Initialize DataLoaders - objects, which sample instances from DataObject-s\n",
    "train_dl = DataLoader(\n",
    "    MyDataset(X_train, y_train),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    MyDataset(X_val, y_val),\n",
    "    batch_size = EVAL_BATCH_SIZE,\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    MyDataset(X_test, y_test),\n",
    "    batch_size = EVAL_BATCH_SIZE,\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "dls = {'train': train_dl, 'val': val_dl, 'test': test_dl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sXvll4ZpsaAV"
   },
   "outputs": [],
   "source": [
    "#add a new layer in the model, which will be modified further\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features = 36, out_features = 1, hidden_size_2 = 128):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_size = int(np.sqrt(in_features + out_features))\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "\n",
    "        self.sequential = nn.Sequential( # NN architecure, where the modules modify the data sequentially\n",
    "            nn.Linear(in_features, self.hidden_size), # Linear transformation\n",
    "            nn.ReLU(), # Activation function \n",
    "            nn.Linear(self.hidden_size, self.hidden_size_2), # Linear transformation\n",
    "            nn.ReLU(), # Activation function \n",
    "            nn.Linear(self.hidden_size_2, self.out_features) # Another Linear transformation\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # In the forward function, you define how your model runs, from input to output \n",
    "        x = self.sequential(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rGKiWLwGsaCg"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED) # Fix random seed to have reproducible weights of model layers\n",
    "\n",
    "model = Model()\n",
    "model.to(DEVICE)\n",
    "\n",
    "loss_fn = nn.MSELoss() # Loss function, which our model will try to minimize\n",
    "# Initialize GD method, which will update the weights of the model\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "# Initialize learning rate scheduler, which will decrease LR according to some rule\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "LveTMa25saF7",
    "outputId": "bec568b6-aca9-408a-ce84-5340ba4a3149"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56fd3eb897ae479eb2956cbd1a9a97c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training loop\n",
    "metrics_dict = {\n",
    "    \"Epoch\": [],\n",
    "    \"Train RMSE\": [],\n",
    "    \"Val RMSE\": [],\n",
    "}\n",
    "\n",
    "# Train loop\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    metrics_dict[\"Epoch\"].append(epoch)\n",
    "    for stage in ['train', 'val']:\n",
    "        with torch.set_grad_enabled(stage == 'train'): # Whether to start building a graph for a backward pass\n",
    "            if stage == 'train':\n",
    "                model.train() # Enable some \"special\" layers (will speak about later)\n",
    "            else:\n",
    "                model.eval() # Disable some \"special\" layers (will speak about later)\n",
    "\n",
    "            loss_at_stage = 0 \n",
    "            for batch in dls[stage]:\n",
    "                x_batch, y_batch = batch\n",
    "                x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "\n",
    "                y_pred = model(x_batch).view(-1) # forward pass: model(x_batch) -> calls forward()\n",
    "                loss = loss_fn(y_pred, y_batch) # Â¡Important! y_pred is always the first arg\n",
    "                if stage == \"train\":\n",
    "                    loss.backward() # Calculate the gradients of all the parameters wrt loss\n",
    "                    optimizer.step() # Update the parameters\n",
    "#                     scheduler.step()\n",
    "                    optimizer.zero_grad() # Zero the saved gradient\n",
    "                with torch.no_grad():\n",
    "                    loss_at_stage += (torch.square((y_pred - y_batch)).sum()).item()\n",
    "            rmse_at_stage = (loss_at_stage / len(dls[stage].dataset)) ** (1/2)\n",
    "            metrics_dict[f\"{stage.title()} RMSE\"].append(rmse_at_stage)\n",
    "            \n",
    "    clear_output(wait=True)\n",
    "    display(pd.DataFrame(metrics_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hab3cqpdvPd"
   },
   "source": [
    "## HuggingFace Trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2I7dDtwQbZej"
   },
   "source": [
    "Instead of implementing full training pipeline we can use the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) class, from the [HugginFace Transformers](https://huggingface.co/docs/transformers/main_classes/trainer) library. For this purpose, we need to define [TrainingArguments](https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/trainer#transformers.TrainingArguments) object with hyperparameters and redefine `compute_loss` function in the Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3AA4NeesxHq"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, set_seed\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#define all hyperparameters in one object TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\", \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    seed=SEED,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "#define custom trainer for computing MSE loss\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        y_true = inputs.get(self.label_names[0])\n",
    "        x = inputs.get(\"x\")\n",
    "        # forward pass\n",
    "        y_preds = model(x).view(-1)\n",
    "        loss = loss_fn(y_preds, y_true)\n",
    "        return (loss, {\"y_preds\": y_preds}) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    y_preds, y_true = eval_preds\n",
    "    y_preds_rescheduled = y_preds * ss2.scale_ + ss2.mean_\n",
    "    y_true_rescheduled = y_true * ss2.scale_ + ss2.mean_\n",
    "    rmse = mean_squared_error(y_true_rescheduled, y_preds_rescheduled, squared=False)\n",
    "    return {\"RMSE\": rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JME45z1xCF8"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_dict({'x': X_train, 'labels': y_train})\n",
    "eval_ds = Dataset.from_dict({'x': X_val, 'labels': y_val})\n",
    "test_ds = Dataset.from_dict({'x': X_test, 'labels': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "id": "Mw4uY6F1sxJz",
    "outputId": "fa53bdcd-42e4-461e-90b4-877fac8459df"
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=GAMMA)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "jrtd4JYgsZSk",
    "outputId": "12ed19c8-9b31-4932-f601-116b4161088b"
   },
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(test_ds)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "3iNXGQO-sZVf",
    "outputId": "9d91f800-d56a-4d1e-aca4-438b92003012"
   },
   "outputs": [],
   "source": [
    "test_preds, test_labels, metrics_ = trainer.predict(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1HVlEWWetMa"
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7B1Qtawwev-t"
   },
   "source": [
    "![picture](https://drive.google.com/uc?export=view&id=19MVBGn0oVwlvyHoAh3cb4fqhYsrATOy5)\n",
    "\n",
    "source: http://primo.ai/index.php?title=Dropout\n",
    "\n",
    "**Note**: Using `Dropout` for regression problems often does not help to prevent overfitting, and can even decrease the performance of the model.\n",
    "\n",
    "When using dropout during training, the activations are scaled in order to preserve their mean value after the dropout layer. The variance, however, is not preserved. \n",
    "\n",
    "This figure shows $R^2$ values for 8 regression datasets.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1PrB6fAbw4CLyAVa_idGLkyjlgorTBCVA\" width=\"400\">\n",
    "\n",
    "source: https://www.researchgate.net/publication/344274687_Effect_of_Dropout_Layer_on_Classical_Regression_Problems\n",
    "\n",
    "real-life example from kaggle: https://www.kaggle.com/competitions/commonlitreadabilityprize/discussion/260729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_4p_VS-evcy"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features = 36, out_features = 1, hidden_size_2 = 128):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_size = int(np.sqrt(in_features + out_features))\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "\n",
    "        self.sequential = nn.Sequential( # NN architecure, where the modules modify the data sequentially\n",
    "            nn.Linear(in_features, self.hidden_size), # Linear transformation\n",
    "            nn.ReLU(), # Activation function \n",
    "            nn.Linear(self.hidden_size, self.hidden_size_2), # Linear transformation\n",
    "            nn.Dropout(p=0.1), #dropout \n",
    "            nn.ReLU(), # Activation function \n",
    "            nn.Linear(self.hidden_size_2, self.out_features) # Another Linear transformation\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # In the forward function, you define how your model runs, from input to output \n",
    "        x = self.sequential(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    set_seed(SEED)\n",
    "    return Model().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "id": "QvyydjVmsFLz",
    "outputId": "87287ac2-1ec8-4f8d-b935-6354737cd04c"
   },
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "5dj2NA9GfEc7",
    "outputId": "7f24bcc5-94ea-4612-d6c2-68c2c913c896"
   },
   "outputs": [],
   "source": [
    "metrics_d = trainer.evaluate(test_ds)\n",
    "metrics_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHgLvZtXdxSZ"
   },
   "source": [
    "## BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKrWjTg32rvO"
   },
   "source": [
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=19FeOZRhIMcEhIlkvd4cAEegyGUgJFNyq\" width=\"400\">\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1vzA1JVW5RiS7OBDBXaN1BcCURM6jmWLW\" width=\"400\">\n",
    "\n",
    "\n",
    "source: https://arxiv.org/pdf/1502.03167.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vE1xcKeGVztY"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features = 36, out_features = 1, hidden_size_2 = 128):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_size = int(np.sqrt(in_features + out_features))\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "\n",
    "        self.sequential = nn.Sequential( # NN architecure, where the modules modify the data sequentially\n",
    "            nn.Linear(in_features, self.hidden_size), # Linear transformation\n",
    "            nn.ReLU(), # Activation function \n",
    "            nn.Linear(self.hidden_size, self.hidden_size_2), # Linear transformation\n",
    "            nn.BatchNorm1d(self.hidden_size_2), #batch normalization for 1D data\n",
    "            nn.ReLU(), # Activation function \n",
    "            nn.Linear(self.hidden_size_2, self.out_features) # Another Linear transformation\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # In the forward function, you define how your model runs, from input to output \n",
    "        x = self.sequential(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "id": "IGeT9SSmVzvs",
    "outputId": "223708a4-8457-4ce7-8014-8a50b48ca85d"
   },
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "0MTT4NUvUan3",
    "outputId": "a7157fe9-d7dc-4790-99f4-38b24c19a0d1"
   },
   "outputs": [],
   "source": [
    "metrics_bn = trainer.evaluate(test_ds)\n",
    "metrics_bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIvZ2M_Qdz3G"
   },
   "source": [
    "## LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRD7abF5c7b3"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1CwobTrNTx5B2JgOhbwpw9c3clKaf5y-x\" width=\"600\">\n",
    "\n",
    "source: https://arxiv.org/pdf/1803.08494.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVLMvlYhd4Ah"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features = 36, out_features = 1, hidden_size_2 = 128):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_size = int(np.sqrt(in_features + out_features))\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "\n",
    "        self.sequential = nn.Sequential( # NN architecure, where the modules modify the data sequentially\n",
    "            nn.Linear(in_features, self.hidden_size), # Linear transformation\n",
    "            nn.ReLU(), # Activation function \n",
    "            nn.Linear(self.hidden_size, self.hidden_size_2), # Linear transformation\n",
    "            nn.LayerNorm(self.hidden_size_2), #layer normalization\n",
    "            nn.ReLU(), # Activation function \n",
    "            nn.Linear(self.hidden_size_2, self.out_features) # Another Linear transformation\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # In the forward function, you define how your model runs, from input to output \n",
    "        x = self.sequential(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "id": "WBsXUielVz6o",
    "outputId": "ccb08d57-c7df-45fd-9602-4f407d19b095"
   },
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "s985iCsVVz9D",
    "outputId": "be04f1c6-9fde-4330-8570-c94fb7d91e48"
   },
   "outputs": [],
   "source": [
    "metrics_ln = trainer.evaluate(test_ds)\n",
    "metrics_ln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyQHk2m6nLAS"
   },
   "source": [
    "## Combine Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nkzzTCygjwY"
   },
   "source": [
    "Let's combine batch normalization and dropout and compare all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zr6lUChrm_z3"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features = 36, out_features = 1, hidden_size_2 = 128):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_size = int(np.sqrt(in_features + out_features))\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "\n",
    "        self.sequential = nn.Sequential( # NN architecure, where the modules modify the data sequentially\n",
    "            nn.Linear(in_features, self.hidden_size), # Linear transformation\n",
    "            nn.ReLU(), # Activation function \n",
    "            nn.Linear(self.hidden_size, self.hidden_size_2), # Linear transformation\n",
    "            nn.BatchNorm1d(self.hidden_size_2), #batch normalization\n",
    "            nn.ReLU(), # Activation function \n",
    "            nn.Dropout(p=0.1), # dropout\n",
    "            nn.Linear(self.hidden_size_2, self.out_features) # Another Linear transformation\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # In the forward function, you define how your model runs, from input to output \n",
    "        x = self.sequential(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "id": "IX0Maya9nHb0",
    "outputId": "4fbcc9d3-bd1f-41b0-bc8f-c736249f91cc"
   },
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, scheduler)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "5ibLbFKZnI_Q",
    "outputId": "0241e49a-5c03-4437-a3dc-eb3e35d83cc7"
   },
   "outputs": [],
   "source": [
    "metrics_final = trainer.evaluate(test_ds)\n",
    "metrics_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4q_Pc1SkpmeN"
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame({'Raw model': [metrics['eval_RMSE']],\n",
    "                    'Model with BatchNorm': [metrics_bn['eval_RMSE']],\n",
    "                    'Model with LayerNorm': [metrics_ln['eval_RMSE']],\n",
    "                    'Model with Dropout': [metrics_d['eval_RMSE']],\n",
    "                    'Final model': [metrics_final['eval_RMSE']]},\n",
    "                   index=['Test RMSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "08-1ZJkjqERk",
    "outputId": "8109c787-825e-44ac-c126-a87e5351c484"
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s05p3vO0g4ZT"
   },
   "source": [
    "We can see that our final model with BatchNorm and Dropout perform better on the test dataset than other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRNYWGhjfYrN"
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSsrwzsqhowJ"
   },
   "source": [
    "This section will be devoted to different initialization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xux1k06wfGI9"
   },
   "outputs": [],
   "source": [
    "#base function for weight matrix initialization\n",
    "def init_weights(m, init_func=torch.nn.init.zeros_):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init_func(m.weight)\n",
    "        \n",
    "        if init_func in [torch.nn.init.zeros_, torch.nn.init.ones_]:\n",
    "            init_func(m.bias)\n",
    "        else:\n",
    "            m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3n5rDAqP_nM"
   },
   "outputs": [],
   "source": [
    "def train_pipeline(train_ds, eval_ds, init_func):\n",
    "    set_seed(SEED)\n",
    "    model = Model()\n",
    "    model.to(DEVICE)\n",
    "    init_weights_func = lambda x: init_weights(x, init_func=init_func)\n",
    "    model.sequential.apply(init_weights_func)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=GAMMA)\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "        optimizers=(optimizer, scheduler)\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "  \n",
    "def get_eval_loss(trainer):\n",
    "    loss = []\n",
    "    logs = trainer.state.log_history\n",
    "    for epoch, log in enumerate(logs[1::2]):\n",
    "        loss.append(log['eval_loss'])\n",
    "    return np.array(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1bliCMuGTXq"
   },
   "source": [
    "## Constant Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhd1QQczGb_g"
   },
   "source": [
    "If all the weights are initialized to zeros, the derivatives will remain the same. As a result, neurons will learn the same features in each iteration. This problem is known as a network failing to break symmetry. And not only zero, but any constant initialization will also produce a poor result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AgFKwnlHny2"
   },
   "source": [
    "### Zeros initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-NzonpShrW7"
   },
   "source": [
    "Set all weight in linear layers equal to zero: $w_i$ = 0\n",
    "\n",
    "In our case, ReLU(0) = 0, then all gradients will be the same and equal to zero. It means that our network will not be training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "id": "9U74Tlb4Oaqh",
    "outputId": "031e0d8a-3de7-439c-8c40-685e84124a53"
   },
   "outputs": [],
   "source": [
    "trainer = train_pipeline(train_ds, eval_ds, torch.nn.init.zeros_)\n",
    "zero_init_loss = get_eval_loss(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C59XImK3H3G-"
   },
   "source": [
    "### Constant initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFbFY4ddiL9f"
   },
   "source": [
    "Set all weight in linear layers equal to constant: $w_i$ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "id": "WJ2guEfiPhm7",
    "outputId": "3649e343-6c91-4098-bc03-7cf7450dc0d0"
   },
   "outputs": [],
   "source": [
    "trainer = train_pipeline(train_ds, eval_ds, torch.nn.init.ones_)\n",
    "ones_init_loss = get_eval_loss(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rPOFq9VGfMn"
   },
   "source": [
    "## Random Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeEqBfGZGh2w"
   },
   "source": [
    "A too-large initialization leads to **exploding** gradients \n",
    "\n",
    "A too-small initialization leads to **vanishing** gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dq7EHRXIMGJ"
   },
   "source": [
    "### Initialization from a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Yy7sv7QYfhj"
   },
   "source": [
    "Generate weights from a normal distribution:\n",
    "$$w_i \\sim \\mathcal{N}(0,\\,1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "id": "bJEDAKtlTgHR",
    "outputId": "622879c1-4dd6-4062-df18-6c4713fd0d01"
   },
   "outputs": [],
   "source": [
    "trainer = train_pipeline(train_ds, eval_ds, torch.nn.init.normal_)\n",
    "norm_init_loss = get_eval_loss(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUKx546jIW-R"
   },
   "source": [
    "### Initialization from a standard uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6CUtkPtYsVg"
   },
   "source": [
    "Generate weights from a standard uniform distribution:\n",
    "\n",
    "$$w_i \\sim \\mathcal{U}(0,\\,1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "id": "ihL1MFIBTgku",
    "outputId": "010ff011-df11-425c-9ed6-6f0f245084f8"
   },
   "outputs": [],
   "source": [
    "trainer = train_pipeline(train_ds, eval_ds, torch.nn.init.uniform_)\n",
    "un_init_loss = get_eval_loss(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LROLEAYrGw9g"
   },
   "source": [
    "## Modern Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLwF93DqIY0Y"
   },
   "source": [
    "### Initialization from a xavier normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E2id2A_brBF"
   },
   "source": [
    "Generate weights from a xavier normal distribution:\n",
    "$$w_i \\sim \\mathcal{N}(0,\\,\\sigma^2)$$\n",
    "$$\\sigma = gain \\sqrt{\\frac{2}{fan_{in}+fan_{out}}}$$\n",
    "\n",
    "source: https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V79gizGniUnE"
   },
   "source": [
    "Theoretically proved that Xavier initialization will perform better when we will use tanh, sigmoid, or logistic activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "id": "AQXALGOufX5o",
    "outputId": "f60147c8-f9e0-4974-f120-d4b06f414197"
   },
   "outputs": [],
   "source": [
    "trainer = train_pipeline(train_ds, eval_ds, torch.nn.init.xavier_normal)\n",
    "xn_init_loss = get_eval_loss(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68w6BXaTIb3p"
   },
   "source": [
    "### Initialization from a xavier uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtsdTkgJcyQX"
   },
   "source": [
    "Generate weights from a xavier uniform distribution:\n",
    "$$w_i \\sim \\mathcal{U}(-a,\\,a)$$\n",
    "$$a = gain \\sqrt{\\frac{6}{fan_{in}+fan_{out}}}$$\n",
    "\n",
    "source: https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "id": "dGNieOGqfX8C",
    "outputId": "c54c4b23-18f8-4b80-c0d1-58aea514a19e"
   },
   "outputs": [],
   "source": [
    "trainer = train_pipeline(train_ds, eval_ds, torch.nn.init.xavier_uniform)\n",
    "xu_init_loss = get_eval_loss(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0CtysivIdcr"
   },
   "source": [
    "### Initialization from a kaiming normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2E7VMCGdJrs"
   },
   "source": [
    "Generate weights from a kaiming normal distribution:\n",
    "$$w_i \\sim \\mathcal{N}(0,\\,\\sigma^2)$$\n",
    "$$\\sigma = \\frac{gain}{\\sqrt{fan_{in}}}$$\n",
    "\n",
    "source: https://arxiv.org/pdf/1502.01852.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jviGXA9tiurm"
   },
   "source": [
    "Kaiming initialization was developed to model with ReLU and its modification activations. Since we use ReLU, we suppose that it is the best choice for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "id": "QJSOyVF7R5fb",
    "outputId": "854f4e7c-932f-40ce-9552-654d67902d98"
   },
   "outputs": [],
   "source": [
    "trainer = train_pipeline(train_ds, eval_ds, torch.nn.init.kaiming_normal_)\n",
    "hen_init_loss = get_eval_loss(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6HWN-f4IgL6"
   },
   "source": [
    "### Initialization from a kaiming uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aezwXHj5dO9C"
   },
   "source": [
    "Generate weights from a kaiming uniform distribution:\n",
    "$$w_i \\sim \\mathcal{U}(-a,\\,a)$$\n",
    "$$a = gain \\sqrt{\\frac{3}{fan_{in}}}$$\n",
    "\n",
    "source:  https://arxiv.org/pdf/1502.01852.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "id": "0gM5fd6BToKd",
    "outputId": "e0d6edbd-8f9a-4d61-d6e7-1fc6cdb06c56"
   },
   "outputs": [],
   "source": [
    "trainer = train_pipeline(train_ds, eval_ds, torch.nn.init.kaiming_uniform_)\n",
    "heu_init_loss = get_eval_loss(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIgk0qijIiog"
   },
   "source": [
    "## Compare different initialization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjefATVEjOvP"
   },
   "source": [
    "Let's compare all results obtained with different activations. As supposed Kaiming initialization works better, but Xavier is almost not far behind the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "EYKxt9YrfYAh",
    "outputId": "d6184efd-771f-4959-8375-cdd41c0d288f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "epochs = list(range(1, NUM_EPOCHS+1))[1:]\n",
    "plt.plot(epochs, zero_init_loss[1:], label='Zeros init.')\n",
    "plt.plot(epochs, ones_init_loss[1:], label='Ones init.')\n",
    "plt.plot(epochs, norm_init_loss[1:], label='Normal init.')\n",
    "plt.plot(epochs, un_init_loss[1:], label='Uniform init.')\n",
    "\n",
    "plt.plot(epochs, xn_init_loss[1:], label='Xavier Normal init.')\n",
    "plt.plot(epochs, xu_init_loss[1:], label='Xavier Uniform init.')\n",
    "\n",
    "plt.plot(epochs, hen_init_loss[1:], label='Kaiming Normal init.')\n",
    "plt.plot(epochs, heu_init_loss[1:], label='Kaiming Uniform init.')\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel('Eval Loss', fontsize=18)\n",
    "plt.xticks(epochs)\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ervR7x5_fYHg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:al] *",
   "language": "python",
   "name": "conda-env-al-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
